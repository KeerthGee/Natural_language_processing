{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081b0281",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bfec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. \n",
    "It includes several tasks such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. \n",
    "The primary objective of text preprocessing is to convert raw text data into a structured format that can be easily analyzed by machine learning algorithms.\n",
    "Text data is often noisy and contains irrelevant information, which can negatively impact the performance of machine learning models. \n",
    "Text preprocessing helps to remove such noise and irrelevant information, thereby improving the accuracy and efficiency of machine learning models. \n",
    "For instance, tokenization breaks down the text into smaller units, such as words or phrases, which can be analyzed more efficiently. \n",
    "Similarly, stemming and lemmatization reduce the inflectional forms of words to their base form, which helps to group similar words together.\n",
    "\n",
    "In conclusion, text preprocessing is an essential step in NLP that helps to transform unstructured text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "It helps to remove noise and irrelevant information from the text, thereby improving the accuracy and efficiency of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3c222",
   "metadata": {},
   "source": [
    "# 2.Describe tokenization in NLP and explain its significance in text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization is the process of breaking down a stream of textual data into smaller units, such as words, phrases, or sentences, called tokens. \n",
    "Tokenization is a fundamental technique in natural language processing (NLP) that is used to preprocess text data before analysis. \n",
    "It is the first step in many NLP tasks, such as text classification, named entity recognition, and sentiment analysis.\n",
    "Tokenization is significant in text processing because it helps to convert unstructured text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "Tokenization breaks down the text into smaller units, which can be analyzed more efficiently. \n",
    "For example, tokenization can help to identify the most frequent words in a text corpus, which can be used to build a word cloud or a bag-of-words model. \n",
    "Tokenization can also help to identify the most common phrases or n-grams in a text corpus, which can be used to build a language model.\n",
    "There are different types of tokenizers available, such as whitespace tokenizer, rule-based tokenizer, and statistical tokenizer. \n",
    "Whitespace tokenizer is the simplest tokenizer that splits the text into words based on whitespace characters. \n",
    "Rule-based tokenizer uses a set of predefined rules to split the text into tokens. \n",
    "Statistical tokenizer uses machine learning algorithms to learn the patterns in the text and split it into tokens.\n",
    "\n",
    "In conclusion, tokenization is a crucial step in NLP that helps to preprocess text data before analysis. \n",
    "It breaks down the text into smaller units, such as words, phrases, or sentences, called tokens, which can be easily analyzed by machine learning algorithms. \n",
    "Tokenization is significant in text processing because it helps to convert unstructured text data into a structured format that can be easily analyzed by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8fc18",
   "metadata": {},
   "source": [
    "# 3.What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming and lemmatization are two text normalization techniques used in natural language processing (NLP) to reduce inflections down to common base root words. \n",
    "Stemming is a rule-based approach that chops off the ends of words, while lemmatization is a canonical dictionary-based approach that returns the base or dictionary form of a word. \n",
    "Both techniques are widely used for text preprocessing in NLP applications such as speech recognition, virtual assistance, and ChatGPT.\n",
    "The primary difference between stemming and lemmatization is that stemming identifies the common root form of a word by removing or replacing word suffixes, while lemmatization identifies the inflected forms of a word and returns its base form. \n",
    "For example, the word \"better\" would be lemmatized as \"good,\" while stemming would reduce it to \"bet\". \n",
    "Lemmatization is more accurate than stemming, but it is slower and requires context analysis. \n",
    "Stemming is faster but can occasionally lead to unmeaningful common base roots. \n",
    "\n",
    "Therefore, the selection of stemming or lemmatization depends on the problem and computational resource availability. \n",
    "In general, if the application requires high accuracy and precision, lemmatization is preferred over stemming. \n",
    "However, if the application requires speed and efficiency, stemming is a better choice . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a9c86",
   "metadata": {},
   "source": [
    "# 4.Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop words are commonly used words in a language that do not carry much meaning and are usually ignored by natural language processing (NLP) algorithms. \n",
    "Examples of stop words in English include \"the,\" \"a,\" \"an,\" \"in,\" etc., \n",
    "The primary role of stop words in text preprocessing is to filter out irrelevant words from the text data, which can improve the accuracy and efficiency of NLP algorithms. \n",
    "Stop words can be removed from the text data before or after tokenization, depending on the application. \n",
    "Stop words can impact NLP tasks in several ways.\n",
    "For instance, stop words can affect the performance of text classification algorithms by introducing noise into the feature space. \n",
    "Similarly, stop words can impact the performance of information retrieval systems by reducing the precision of search results. \n",
    "However, stop words can also be useful in some NLP tasks, such as sentiment analysis, where they can help to identify the polarity of a sentence. \n",
    "\n",
    "In conclusion, stop words are commonly used words in a language that do not carry much meaning and are usually ignored by NLP algorithms. \n",
    "They play a crucial role in text preprocessing by filtering out irrelevant words from the text data, which can improve the accuracy and efficiency of NLP algorithms. \n",
    "However, stop words can also impact the performance of NLP tasks in different ways, depending on the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e337091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text , stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This is a sample text, with stop words\"\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7991c8",
   "metadata": {},
   "source": [
    "# 5.How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a26d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing punctuation is an essential step in text preprocessing in natural language processing (NLP) that involves removing all the punctuation marks from the text data. \n",
    "Punctuation marks include characters such as commas, periods, question marks, exclamation marks, and others. \n",
    "The primary benefit of removing punctuation is that it helps to reduce the dimensionality of the text data, which can improve the accuracy and efficiency of NLP algorithms. \n",
    "Punctuation marks are often irrelevant to the meaning of the text and can introduce noise into the feature space. \n",
    "Removing punctuation can also help to standardize the text data and make it easier to analyze. \n",
    "For example, consider the following two sentences:\n",
    "- \"I am happy!\"\n",
    "- \"I am happy.\"\n",
    "The only difference between these two sentences is the presence of the exclamation mark in the first sentence. \n",
    "However, this difference can significantly impact the sentiment analysis of the text. \n",
    "By removing the exclamation mark, we can standardize the text data and make it easier to analyze .\n",
    "\n",
    "In conclusion, removing punctuation is a crucial step in text preprocessing in NLP that helps to reduce the dimensionality of the text data, standardize the text data, \n",
    "and improve the accuracy and efficiency of NLP algorithms ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0b4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text with punctuations\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is a sample text, with punctuations!\"\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "text = remove_punctuation(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ed0f9",
   "metadata": {},
   "source": [
    "# 6.Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lowercasing is a simple and effective technique of text preprocessing that involves converting every single token of the input text to lowercase. \n",
    "This technique helps in dealing with sparsity issues in the dataset and is applicable to most text mining and NLP problems. \n",
    "Lowercasing can also help with consistency of expected output.\n",
    "The importance of lowercase conversion in text preprocessing lies in the fact that it helps to standardize the text data and reduce the dimensionality of the feature space. \n",
    "Text data often contains words in different cases, such as uppercase, lowercase, or mixed case, which can introduce noise into the feature space and negatively impact the performance of machine learning models. \n",
    "By converting all the words to lowercase, we can standardize the text data and make it easier to analyze.\n",
    "For example, consider the following two sentences:\n",
    "- \"The quick brown fox jumps over the Lazy Dog.\"\n",
    "- \"THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\"\n",
    "These two sentences are identical in meaning but differ in case. \n",
    "By converting all the words to lowercase, we can standardize the text data and make it easier to analyze.\n",
    "\n",
    "In conclusion, lowercase conversion is a common step in NLP tasks that helps to standardize the text data and reduce the dimensionality of the feature space. \n",
    "It is a simple and effective technique of text preprocessing that can improve the accuracy and efficiency of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636ef78",
   "metadata": {},
   "source": [
    "# 7.Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ff469",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorization in natural language processing (NLP) refers to the process of converting textual data, such as sentences or documents, into numerical vectors that can be used for data analysis, machine learning, and other computational tasks. \n",
    "Vectorization is a crucial step in NLP that helps to convert unstructured text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "CountVectorizer is a popular vectorization technique in NLP that is used to transform a collection of text documents into a numerical representation. \n",
    "CountVectorizer tokenizes the text data by breaking it down into words and then creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. \n",
    "The value of each cell is nothing but the count of the word in that particular text sample. \n",
    "This way of representation is known as a sparse matrix. \n",
    "CountVectorizer is a simple and effective technique of text preprocessing that can improve the accuracy and efficiency of machine learning models.\n",
    "Other vectorization techniques in NLP include TF-IDF (term frequency-inverse document frequency) and word embeddings. TF-IDF is designed to get how much the words are relevant in the corpus, while word embeddings represent words or documents as points in high-dimensional space, where each dimension represents a particular feature or characteristic of the text.\n",
    "\n",
    "In conclusion, vectorization is a crucial step in NLP that helps to convert unstructured text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "CountVectorizer is a popular vectorization technique in NLP that can improve the accuracy and efficiency of machine learning models. Other vectorization techniques in NLP include TF-IDF and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815ab18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['This is the first document.', 'This is the second document.', 'And, the third one.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db79a96",
   "metadata": {},
   "source": [
    "# 8.Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7345b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalization is a crucial step in natural language processing (NLP) that involves transforming unstructured text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "Normalization techniques are used to standardize and clean keywords or phrases in text data, in order to make them more usable for NLP tasks. \n",
    "\n",
    "Normalization techniques include:\n",
    "1.Case normalization: converting all text to lowercase or uppercase to standardize the text.\n",
    "2. Punctuation removal: removing special characters and punctuation marks from the text.\n",
    "3. Stop word removal: removing common words with little meaning, such as \"the\" and \"a\".\n",
    "4. Stemming: reducing inflectional forms of words to their base form.\n",
    "5. Lemmatization: returning the base or dictionary form of a word.\n",
    "6. Tokenization: breaking down a stream of textual data into smaller units, such as words, phrases, or sentences, called tokens.\n",
    "7. Replacing synonyms and abbreviations to their full form: normalizing the text by replacing synonyms and abbreviations with their full form.\n",
    "8. Removing numbers and symbols: normalizing the text by removing numbers and symbols.\n",
    "9. Handling whitespace: normalizing the text by handling whitespace.\n",
    "10.Expanding contractions: normalizing the text by expanding contractions, such as \"don't\" to \"do not\".\n",
    "11.Handling unicode characters: normalizing the text by handling accented letters and some punctuation.\n",
    "12.Number words -> numeric: normalizing the text by converting number words to numeric.\n",
    "\n",
    "These techniques help to convert raw text data into a structured format that can be easily analyzed by machine learning algorithms. \n",
    "They help to remove noise and irrelevant information from the text, thereby improving the accuracy and efficiency of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b99e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'to sleep or not to sleep, that is the question'\n"
     ]
    }
   ],
   "source": [
    "text = \"'To Sleep Or NOT to SLEep, THAT is THe Question'\"\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "lower_case = lower_case(text) #converts everything to lowercase\n",
    "print(lower_case) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
